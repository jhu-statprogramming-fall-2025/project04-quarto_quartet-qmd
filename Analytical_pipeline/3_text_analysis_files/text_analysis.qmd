---
title: "Text Analysis"
format:
  html:
    self-contained: true
date: 2025-12-05
---

*NIH grant application* <https://grants.nih.gov/grants/how-to-apply-application-guide/forms-i/general/g.200-sf-424-%28r%26r%29-form.htm?utm_source=chatgpt.com#11>



- Title: <= 200
- Project Summary/Abstract: Your Project Summary/Abstract helps the public understand the value of NIH-funded research. We advise you to write this attachment at a level that would be informative to others working in the same or related fields and understandable to a science-literate reader.
- Project Narrative: Your Project Narrative communicates your project’s public health relevance. We advise you to use plain language that would be understandable by a general audience.
("Project Summary/Abstract" vs "Project Narrative" <https://websites.umass.edu/officeofresearch/2019/07/08/national-institutes-of-health-grants-project-summary-abstract-vs-project-narrativr/?utm_source=chatgpt.com>)




*NSF grant application* <https://www.nsf.gov/policies/pappg/24-1?utm_source=chatgpt.com>
- Title: 
- Abstract: Overview、Intellectual Merit、Broader Impacts


# 1. Load combined data and packages


```{r}
#｜message: FALSE
library(dplyr)
library(tidyr)
library(tidytext)
library(stringr)
library(ggplot2)
library(scales)
library(SnowballC)
```

```{r}
data <- read.csv("~/Downloads/merged_data.csv")
str(data)
table(data$Notification_year)
table(data$Activity.Category)
head(data$Title, n=10)
nrow(data[data$Source == "NIH",])
nrow(data[data$Source == "NSF",])
head(data[data$Source == "NIH",]$Abstract, n = 10)
head(data[data$Source == "NSF",]$Abstract, n = 10)
```

## Clean NSF Abstract

```{r}
target <- "\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."


data_checked <- data %>%
  mutate(
    text_trim = str_trim(Abstract),
    has_award_sentence = str_ends(Abstract, fixed(target))
  )

data_checked <- data[data$Source =="NSF",] %>%
  mutate(
    text_trim = str_trim(Abstract),
    has_award_sentence = str_ends(Abstract, fixed(target))
  )

data_checked %>%
  filter(!has_award_sentence) %>%
  select(Abstract) %>%
  head()

table(data_checked$has_award_sentence)

```

```{r}

```
```{r}
data <- data %>%
  mutate(
    Abstract_clean = if_else(
      # 如果以这段固定文本结尾
      str_ends(Abstract, fixed(target)),
      # 就从末尾把这段剪掉
      str_sub(Abstract, 1, nchar(Abstract) - nchar(target)),
      # 否则保持不变
      Abstract
    )
  )
str(data)

data_checked <- data %>%
  mutate(
    text_trim = str_trim(Abstract_clean),
    has_award_sentence = str_ends(Abstract_clean, fixed(target))
  )
table(data_checked$has_award_sentence)


data$Abstract <- NULL

saveRDS(data, "~/Downloads/data_abstract_clean.rds")
```

## Stop words

```{r}
stop_words <- tidytext::stop_words %>% as_tibble()

# No info words
# custom_stop <- c("project")
custom_stop <- c("project", "study", "research", "health", "grant", "award", "summary", "development", "role", "dissertation", "doctoral", "conference", "language", "understanding", "learning", "risk", "function", "regulation", "treatment", "based", "training", "collaborative", "career", "impact", "change", "dynamics", "eager", "center", "target", "nsf", "rui","sai", "goals", "proposal", "proposed", "including", "specific", "develop", "provide", "care", "program", "systems", "targeting")
```


# 2. Single word frequency

## 2.1 Function for counting words
```{r}
count_words_by_field <- function(df, text_col, field_name) {
  df %>%
    select(Source, text = {{ text_col }}) %>%
    filter(!is.na(text), text != "") %>%
    tidytext::unnest_tokens(word, text) %>%
    mutate(word = str_to_lower(word)) %>%
    anti_join(stop_words, by = "word") %>%
    filter(!str_detect(word, "^[0-9]+$")) %>%
    filter(!word %in% custom_stop) %>%
    count(Source, word, sort = TRUE) %>%
    mutate(field = field_name)
}

title_words    <- count_words_by_field(data, Title,    "title")
abstract_words <- count_words_by_field(data, Abstract_clean, "abstract")
```

```{r}
title_words[title_words$Source == "NIH",][1:50,2]
title_words[title_words$Source == "NSF",][1:50,2]
abstract_words[abstract_words$Source == "NIH",][1:50,2]
abstract_words[abstract_words$Source == "NSF",][1:50,2]
```

## 2.2 Frequency plot

```{r}
plot_top_terms_prop <- function(words_df, field_name, top_n = 15) {
  df <- words_df %>%
    filter(field == field_name) %>%
    group_by(Source) %>%
    mutate(
      total = sum(n),
      freq  = n / total
    ) %>%
    slice_max(freq, n = top_n, with_ties = FALSE) %>% # freq according to Top n
    ungroup() %>%
    mutate(word = tidytext::reorder_within(word, freq, Source))
  
  ggplot(df, aes(x = word, y = freq)) +
    geom_col() +
    facet_wrap(~ Source, scales = "free_y") +
    coord_flip() +
    tidytext::scale_x_reordered() +
    scale_y_continuous(labels = percent_format(accuracy = 0.01)) +
    labs(
      title = paste("Top", top_n, "terms in", field_name, "by source (relative frequency)"),
      x = NULL,
      y = "Proportion of all tokens"
    )
}

words_all <- bind_rows(title_words, abstract_words)

plot_top_terms_prop(words_all, "title", top_n = 15)
plot_top_terms_prop(words_all, "abstract", top_n = 15)

```

NIH titles emphasize diseases, organs, and clinical outcomes, whereas NSF titles highlight fundamental biological mechanisms, social and environmental systems, and computational methods.


## 2.3 log2(freqNIH/freqNSF)

```{r}
plot_logratio <- function(words_df, field_name, top_n = 15) {
  eps <- 1e-7
  
  freq <- words_df %>%
    filter(field == field_name) %>%
    group_by(Source) %>%
    mutate(
      total = sum(n),
      freq  = n / total
    ) %>%
    ungroup() %>%
    select(Source, word, freq) %>%
    pivot_wider(names_from = Source, values_from = freq, values_fill = 0)
  
  # 现在有列 NIH 和 NSF
  word_logratio <- freq %>%
    mutate(
      log_ratio = log2((NIH + eps) / (NSF + eps)),
      max_freq  = pmax(NIH, NSF)
    ) %>%
    # 去掉极端罕见的词，防止噪音（阈值可以调整）
    filter(max_freq > 1e-6)
  
  nih_pref <- word_logratio %>%
    arrange(desc(log_ratio)) %>%
    slice_head(n = top_n) %>%
    mutate(pref = "More NIH")
  
  nsf_pref <- word_logratio %>%
    arrange(log_ratio) %>%
    slice_head(n = top_n) %>%
    mutate(pref = "More NSF")
  
  pref_words <- bind_rows(nih_pref, nsf_pref) %>%
    mutate(word = reorder(word, log_ratio))
  
  ggplot(pref_words, aes(x = word, y = log_ratio, fill = pref)) +
    geom_col() +
    coord_flip() +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(
      title = paste("Words characteristic of NIH vs NSF in", field_name),
      x = NULL,
      y = "log2(freq_NIH / freq_NSF)"
    )
}
```

```{r}
# 标题：哪些词更“NIH 味”，哪些更“NSF 味”
plot_logratio(words_all, field_name = "title", top_n = 15)

# 摘要：哪些词更“NIH 味”，哪些更“NSF 味”
plot_logratio(words_all, field_name = "abstract", top_n = 15)
```

## 2.4 Theme

```{r}
theme_lexicon <- tibble::tibble(
  word = c(
    # disease 相关
    "disease","cancer","hiv","infection","tumor","chronic","alzheimer's",
    "lung","injury","alcohol","ad","opioids",
    
    # clinical / intervention / outcome
    "clinical","patients","patient","therapy","therapeutic","trial",
    "intervention","outcomes","prevention","care","risk",
    
    # methods / data / tools / infrastructure
    "data","model","models","analysis","methods","computational",
    "tools","infrastructure","networks","spatial","processing",
    
    # education / training / people / scientists
    "students","student","undergraduate","graduate","training",
    "opportunities","career","doctoral","dissertation","researchers",
    "science","scientific","knowledge",
    
    # community / public / social / environment
    "social","communities","community","people","public","local",
    "diverse","environment","environmental","stress","resilience",
    
    # basic biology / mechanisms
    "cell","cells","molecular","protein","proteins","gene","genes",
    "genetic","rna","dna","biology","biological","cellular","brain",
    "neural","plant","mechanisms","mechanism"
  ),
  theme = c(
    rep("disease",  length(c("disease","cancer","hiv","infection","tumor","chronic","alzheimer's","lung","injury","alcohol","ad","opioids"))),
    rep("clinical", length(c("clinical","patients","patient","therapy","therapeutic","trial","intervention","outcomes","prevention","care","risk"))),
    rep("methods",  length(c("data","model","models","analysis","methods","computational","tools","infrastructure","networks","spatial","processing"))),
    rep("education",length(c("students","student","undergraduate","graduate","training","opportunities","career","doctoral","dissertation","researchers","science","scientific","knowledge"))),
    rep("community",length(c("social","communities","community","people","public","local","diverse","environment","environmental","stress","resilience"))),
    rep("basic_bio",length(c("cell","cells","molecular","protein","proteins","gene","genes","genetic","rna","dna","biology","biological","cellular","brain","neural","plant","mechanisms","mechanism")))
  )
)
```

```{r}
words_theme <- words_all %>%
  inner_join(theme_lexicon, by = "word") %>%   # 只保留在词典里的词
  group_by(field, Source, theme) %>%
  summarise(n = sum(n), .groups = "drop") %>%
  group_by(field, Source) %>%
  mutate(
    total = sum(n),
    prop  = n / total
  ) %>%
  ungroup()
```

```{r}
ggplot(words_theme %>% filter(field == "title"),
       aes(x = Source, y = prop, fill = theme)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    title = "Theme composition in titles (NIH vs NSF)",
    x = NULL,
    y = "Proportion (within themed words)"
  )
```

```{r}
ggplot(words_theme %>% filter(field == "abstract"),
       aes(x = Source, y = prop, fill = theme)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    title = "Theme composition in abstracts (NIH vs NSF)",
    x = NULL,
    y = "Proportion (within themed words)"
  )
```

这太奇怪了！！要精读各20个Abstract，看看到底写的是什么

你跑完图之后，大概率可以写出类似的话（用你看到的具体词填空）：

Signature words. Log-ratio analysis showed that NIH titles and abstracts are enriched for terms such as disease, cancer, HIV, clinical, patients, outcomes, whereas NSF documents overuse words like students, communities, infrastructure, tools, methods. This pattern reflects NIH’s emphasis on diseases and clinical outcomes and NSF’s focus on education, community engagement, and research infrastructure.

Thematic composition. When grouping words into coarse themes (disease, clinical, methods/data, education, community, basic biology), NIH documents allocate a much larger share of their vocabulary to disease- and clinical-related terms, especially in abstracts. By contrast, NSF documents devote a greater proportion of their language to methods/data, education/training, and community/public themes, consistent with NSF’s role in supporting fundamental science, training, and infrastructure.

## 2.4 Length analysis (DISCARD)

```{r}
len_df <- text_all %>%
  mutate(
    title_chars    = nchar(title),
    abstract_chars = nchar(abstract)
  )

# Title length
len_df %>%
  ggplot(aes(x = source, y = title_chars)) +
  geom_boxplot() +
  labs(title = "Title length by source", y = "Number of characters", x = NULL)

# Abstract length
len_df %>%
  ggplot(aes(x = source, y = abstract_chars)) +
  geom_boxplot() +
  labs(title = "Abstract length by source", y = "Number of characters", x = NULL)

len_df %>%
  ggplot(aes(x = source, y = abstract_chars)) +
  geom_boxplot(outlier.shape = NA) +
  coord_cartesian(ylim = c(0, 5000)) +   # 0–5000, to exclude outliers
  labs(
    title = "Abstract length by source",
    y = "Number of characters",
    x = NULL
  )
```


## 2.5 Title–abstract relationship: overlapping info between abstract and title

NIH projects tend to show higher overlap between title and abstract vocabulary compared with NSF projects, suggesting that NSF abstracts introduce more novel technical detail beyond what is captured in the title.
```{r}
library(purrr)

set.seed(1)
sample_idx <- sample(seq_len(nrow(text_all)), size = 20000)

jaccard_df <- text_all[sample_idx, ] %>%
  mutate(
    title_words    = map(title, ~ str_split(.x, "\\s+")[[1]] %>% tolower()),
    abstract_words = map(abstract, ~ str_split(.x, "\\s+")[[1]] %>% tolower()),
    inter = map2_int(title_words, abstract_words,
                     ~ length(intersect(.x, .y))),
    union = map2_int(title_words, abstract_words,
                     ~ length(union(.x, .y))),
    jaccard = ifelse(union == 0, NA_real_, inter / union)
  )

ggplot(jaccard_df, aes(x = source, y = jaccard)) +
  geom_boxplot() +
  labs(title = "Jaccard similarity between title and abstract (sample)",
       y = "Jaccard index", x = NULL)
```


```{r}
cat(text_all[18, 2][[1]], "\n")
```






# 3. Bigrams analysis
## 3.1 Abstract and count bigrams

```{r}
# 1. 把 Title / Abstract 放到同一列，保留一些关键信息方便以后用
text_long <- data %>%
  select(
    ID,
    Source,
    Year,
    Title,
    Abstract_clean
  ) %>%
  pivot_longer(
    cols = c(Title, Abstract_clean),
    names_to = "field",       # "Title" or "Abstract_clean"
    values_to = "text"
  ) %>%
  filter(!is.na(text), text != "")


# 最原始的bigram tokens
bigrams_raw <- text_long %>%
  tidytext::unnest_tokens(bigram, text, token = "ngrams", n = 2)

str(bigrams_raw)
saveRDS(bigrams_raw, "~/Downloads/bigrams_raw.rds")
```

```{r}
bigrams_clean_tokens <- text_long %>%
  tidytext::unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  # 拆成两个词
  separate(bigram, into = c("w1", "w2"), sep = " ") %>%
  mutate(
    w1 = str_to_lower(w1),
    w2 = str_to_lower(w2)
  ) %>%
  # 去停用词：只要有一个是停用词，就丢掉
  anti_join(stop_words, by = c("w1" = "word")) %>%
  anti_join(stop_words, by = c("w2" = "word")) %>%
  # 去掉纯数字的 bigram
  filter(
    !str_detect(w1, "^[0-9]+$"),
    !str_detect(w2, "^[0-9]+$")
  ) %>%
  # 去掉自定义“没信息”的词
  filter(
    !w1 %in% custom_stop,
    !w2 %in% custom_stop
  ) %>%
  # 再合回 bigram 文本
  unite("bigram", w1, w2, sep = " ")

str(bigrams_clean_tokens)
saveRDS(bigrams_clean_tokens, "~/Downloads/bigrams_clean_tokens.rds")
```




```{r}
## Cleaned
bigrams_counts <- bigrams_clean_tokens %>%
  count(Source, field, bigram, sort = TRUE)

bigrams_counts_year <- bigrams_clean_tokens %>%
  count(Source, Year, field, bigram, sort = TRUE)
# 用这个就可以做 2020–2025 bigram 热度趋势


## Raw
bigrams_raw_counts <- bigrams_raw %>%
  count(Source, field, bigram, sort = TRUE)

bigrams_raw_counts_year <- bigrams_raw %>%
  count(Source, Year, field, bigram, sort = TRUE)
```

```{r}
bigrams_clean_tokens %>% head()
bigrams_counts %>% head(20)
bigrams_counts_year |> head(20)
```


```{r}
count_bigrams_by_field <- function(df, text_col, field_name) {
  df %>%
    select(Source, text = {{ text_col }}) %>%
    filter(!is.na(text), text != "") %>%
    tidytext::unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, into = c("w1", "w2"), sep = " ", remove = FALSE) %>%
    mutate(
      w1 = str_to_lower(w1),
      w2 = str_to_lower(w2)
    ) %>%
    # remove stop words
    anti_join(stop_words, by = c("w1" = "word")) %>%
    anti_join(stop_words, by = c("w2" = "word")) %>%
    # remove number bigram
    filter(!str_detect(w1, "^[0-9]+$"),
           !str_detect(w2, "^[0-9]+$")) %>%
    # remove non-informative bigrams
    filter(
      !w1 %in% custom_stop,
      !w2 %in% custom_stop
    ) %>%
    # recombine to form bigrams
    unite("bigram", w1, w2, sep = " ", remove = TRUE) %>%
    count(source, bigram, sort = TRUE) %>%
    mutate(field = field_name)
}

# title_bigrams = readRDS("~/Downloads/title_bigrams.rds")
 title_bigrams    <- count_bigrams_by_field(data, Title,    "title")
head(title_bigrams)
# saveRDS(title_bigrams, "~/Downloads/title_bigrams.rds")
abstract_bigrams <- count_bigrams_by_field(data, abstract, "abstract")
# saveRDS(abstract_bigrams, "~/Downloads/abstract_bigrams.rds")
# load(abstract_bigrams.rds)
```

## 3.2 Frequency plot

```{r}
plot_top_bigrams_prop <- function(bigram_df, field_name, top_n = 15) {
  df <- bigram_df %>%
    filter(field == field_name) %>%
    group_by(source) %>%
    mutate(
      total = sum(n),
      freq  = n / total
    ) %>%
    slice_max(freq, n = top_n, with_ties = FALSE) %>%
    ungroup() %>%
    mutate(bigram = tidytext::reorder_within(bigram, freq, source))
  
  ggplot(df, aes(x = bigram, y = freq)) +
    geom_col() +
    facet_wrap(~ source, scales = "free_y") +
    coord_flip() +
    tidytext::scale_x_reordered() +
    scale_y_continuous(labels = percent_format(accuracy = 0.01)) +
    labs(
      title = paste("Top", top_n, "bigrams in", field_name, "by source (relative frequency)"),
      x = NULL,
      y = "Proportion of all bigrams"
    )
}

# Title
plot_top_bigrams_prop(title_bigrams,
                      field_name = "title", top_n = 15)

# Abstract
# plot_top_bigrams_prop(bind_rows(title_bigrams, abstract_bigrams),
#                      field_name = "abstract", top_n = 15)
```


## 3.3 log2(freqNIH/freqNSF)


```{r}
compute_bigram_logratio_plot <- function(bigram_df, field_name, top_n = 15) {
  eps <- 1e-7
  
  freq <- bigram_df %>%
    filter(field == field_name) %>%
    group_by(source) %>%
    mutate(
      total = sum(n),
      freq  = n / total
    ) %>%
    ungroup() %>%
    select(source, bigram, freq) %>%
    pivot_wider(names_from = source, values_from = freq, values_fill = 0)
  
  bigram_logratio <- freq %>%
    mutate(
      log_ratio = log2((NIH + eps) / (NSF + eps)),
      max_freq  = pmax(NIH, NSF)
    ) %>%
    filter(max_freq > 1e-6)
  
  nih_pref <- bigram_logratio %>%
    arrange(desc(log_ratio)) %>%
    slice_head(n = top_n) %>%
    mutate(pref = "More NIH")
  
  nsf_pref <- bigram_logratio %>%
    arrange(log_ratio) %>%
    slice_head(n = top_n) %>%
    mutate(pref = "More NSF")
  
  pref_bigrams <- bind_rows(nih_pref, nsf_pref) %>%
    mutate(bigram = reorder(bigram, log_ratio))
  
  ggplot(pref_bigrams, aes(x = bigram, y = log_ratio, fill = pref)) +
    geom_col() +
    coord_flip() +
    labs(
      title = paste("Bigrams characteristic of NIH vs NSF in", field_name),
      x = NULL,
      y = "log2(freq_NIH / freq_NSF)"
    ) +
    geom_hline(yintercept = 0, linetype = "dashed")
}

bigrams_all <- title_bigrams

# Title
compute_bigram_logratio_plot(bigrams_all, field_name = "title", top_n = 15)

# Abstract
# compute_bigram_logratio_plot(bigrams_all, field_name = "abstract", top_n = 15)
```

## 3.4 Buzz word analysis

```{r}
# buzz words we are interested in
buzz_bigrams <- c(
  "machine learning",
  "artificial intelligence",
  "deep learning",
  "remote sensing",
  "wearable sensors",
  "health disparities",
  "public health",
  "mental health",
  "vaccine effectiveness"
)

buzz_df <- bigrams_all %>%
  filter(bigram %in% buzz_bigrams) %>%
  group_by(field, source, bigram) %>%
  summarise(n = sum(n), .groups = "drop") %>%
  group_by(field, source) %>%
  mutate(freq = n / sum(n)) %>%
  ungroup()

ggplot(buzz_df, aes(x = bigram, y = freq, fill = source)) +
  geom_col(position = "dodge") +
  facet_wrap(~ field, scales = "free_y") +
  coord_flip() +
  scale_y_continuous(labels = percent_format(accuracy = 0.01)) +
  labs(
    title = "Selected buzzword bigrams in NIH vs NSF (title & abstract)",
    x = NULL,
    y = "Proportion of all bigrams"
  )
```
